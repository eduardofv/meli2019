{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MercadoLibre 2019\n",
    "## BERT on Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"meli-BERTK\" \n",
    "EXPERIMENT_VERSION = \"v2_2\"\n",
    "LOG_DIR = \"../logs/BERT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version log\n",
    "\n",
    "### v2_2\n",
    "\n",
    "- DS reliable (1M+)\n",
    "- **Los resultados son buenos para un eval set de solo reliable (ver abajo) pero malos para uno general**\n",
    "    - Probado en `eval_5k` con notebook de evaluacion\n",
    "    - Correctas: 1129/5000 = 0.2258\n",
    "    - BAS = 0.16832066415719524\n",
    "\n",
    "### v2_1\n",
    "\n",
    "- Dataset 1M\n",
    "- Desde la 2_0 ya se corrió con sparse_categorical_crossentropy\n",
    "\n",
    "### v2_0\n",
    "\n",
    "Origen: v1_2\n",
    "\n",
    "- Se separó la tokenización y creación de features para optimizar memoria\n",
    "- Correccion de lo que parece un bug en la conversión de etiquetas\n",
    "\n",
    "### v1_2\n",
    "\n",
    "- rel y unrel, \n",
    "- bert uncased, \n",
    "- 1M/train-full \n",
    "- max_seq_len = 64\n",
    "- val y test set 0.025\n",
    "\n",
    "### v1_1\n",
    "\n",
    "Solo con campos \"reliable\"\n",
    "\n",
    "### v1_0_1\n",
    "\n",
    "Misma versión con training set de 500,000, 7 épocas\n",
    "\n",
    "### v1_0 \n",
    "\n",
    "Viene de movie2vec-Plot_BERTK-v1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "#import re\n",
    "import time\n",
    "#import h5py\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense \n",
    "from tensorflow.keras.layers import Dropout, Input, Concatenate, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "#from bert.tokenization import FullTokenizer\n",
    "#from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize env\n",
    "#seeds to make reproducible\n",
    "#todo: check reproducibility\n",
    "np.random.seed(12347)\n",
    "tf.set_random_seed(12347)\n",
    "\n",
    "pd.options.display.max_rows = 7\n",
    "\n",
    "#filenames and directories\n",
    "TOKENIZED_DATASET_FN = \"../data/tokenized/train-reliable-features-uncased\"\n",
    "SAVED_MODEL_DIR = \"../saved_models/\"\n",
    "\n",
    "#set some parameters on how data will be used\n",
    "# how much data will reserve for test set (of the DS prop to use) (0.10)\n",
    "TEST_SET_SPLIT = 0.005\n",
    "# how much of the data will be used for validation (of the DS prop to use) (0.05)\n",
    "VALIDATION_SET_SPLIT = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare tokenized datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(suffix):\n",
    "    with open(TOKENIZED_DATASET_FN+suffix, \"rb\") as fin:\n",
    "        data = pickle.load(fin)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_load = ['input_ids', 'input_masks', 'segment_ids', \n",
    "                'cat_dict', 'inv_cat_dict', 'info']\n",
    "data = {}\n",
    "for d in data_to_load:\n",
    "    data[d] = load(f\"-{d}.pickle\")\n",
    "labels_index = load(\"-labels-index.pickle\")\n",
    "#data['labels'] = to_categorical(labels_index)\n",
    "data['labels'] = labels_index\n",
    "data['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chk: SHUFFLE!!\n",
    "num_samples = len(data['input_ids'])\n",
    "\n",
    "num_test_samples = int(num_samples * TEST_SET_SPLIT)\n",
    "num_training_samples = num_samples - num_test_samples\n",
    "\n",
    "#OJO!!!! V2_2 con pocos ejemplos!!!!\n",
    "num_training_samples = 500000\n",
    "num_test_samples = 5000\n",
    "\n",
    "print(f\"Training samples: {num_training_samples}\")\n",
    "print(f\"Test samples: {num_test_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, bert_path, n_fine_tune_layers=10, **kwargs):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.bert_path = bert_path\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path,\n",
    "            trainable=self.trainable,\n",
    "            name=\"{}_module\".format(self.name)\n",
    "        )\n",
    "\n",
    "        trainable_vars = self.bert.variables\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "            \n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "            \"pooled_output\"\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0919 18:32:32.664514 140616766973760 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "IN_IDS (InputLayer)             [(None, 32)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "IN_MASKS (InputLayer)           [(None, 32)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "IN_SEGM (InputLayer)            [(None, 32)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, 768)          110104890   IN_IDS[0][0]                     \n",
      "                                                                 IN_MASKS[0][0]                   \n",
      "                                                                 IN_SEGM[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "DEN_1024 (Dense)                (None, 1024)         787456      bert_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1024)         0           DEN_1024[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "DEN_512 (Dense)                 (None, 512)          524800      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           DEN_512[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "DEN_256 (Dense)                 (None, 256)          131328      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "DEN_OUT (Dense)                 (None, 1394)         358258      DEN_256[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 111,906,732\n",
      "Trainable params: 4,751,730\n",
      "Non-trainable params: 107,155,002\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = data['info']['max_seq_length']\n",
    "output_dim = len(data['labels'].unique())#len(data['labels'][0])\n",
    "\n",
    "in_id = Input(shape=(max_seq_length,), name=\"IN_IDS\")\n",
    "in_mask = Input(shape=(max_seq_length,), name=\"IN_MASKS\")\n",
    "in_segment = Input(shape=(max_seq_length,), name=\"IN_SEGM\")\n",
    "\n",
    "inputs = [in_id, in_mask, in_segment]\n",
    "fo = BertLayer(bert_path=data['info']['BERT_PATH'], \n",
    "               n_fine_tune_layers=3)(inputs)\n",
    "#fo = Dense(1024, activation=\"relu\", name=\"DEN_1024\")(fo)\n",
    "#fo = Dropout(0.5)(fo)\n",
    "#fo = Dense(512, activation=\"relu\", name=\"DEN_512\")(fo)\n",
    "#fo = Dropout(0.5)(fo)\n",
    "#fo = Dense(256, activation=\"relu\", name=\"DEN_256\")(fo)\n",
    "#fo = Dense(output_dim, activation=\"softmax\", name=\"DEN_OUT\")(fo)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=fo)\n",
    "model.compile(loss='sparse_categorical_crossentropy',#'categorical_crossentropy', \n",
    "              optimizer='adam')#, learning_rate=0.003)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.6.8 (default, Jan 14 2019, 11:02:34) \n",
      "[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]\n",
      "Tensorflow version: 1.14.0\n",
      "Keras version: 2.2.4-tf\n",
      "Embeddings: https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\n",
      "RUNID: meli-BERTK-v2_2-190919_1832\n"
     ]
    }
   ],
   "source": [
    "#run params\n",
    "runid = \"%s-%s-%s\"%\\\n",
    "    (EXPERIMENT_NAME,\n",
    "     EXPERIMENT_VERSION,\n",
    "     time.strftime(time.strftime('%y%m%d_%H%M',time.localtime())))\n",
    "\n",
    "#Create saved model dir     \n",
    "directory = SAVED_MODEL_DIR+\"/\"+runid\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "print(\"Python: \"+str(sys.version))\n",
    "print(\"Tensorflow version: \"+tf.VERSION)\n",
    "print(\"Keras version: \"+tf.keras.__version__)\n",
    "print(\"Embeddings: \"+data['info']['BERT_PATH'])\n",
    "print(\"RUNID: \"+runid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.local_variables_initializer())\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Start:20190919_1832\n",
      "Train on 497500 samples, validate on 2500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0919 18:32:35.822645 140616766973760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "497408/497500 [============================>.] - ETA: 0s - loss: 4.6598\n",
      "Epoch 00001: val_loss improved from inf to 3.03765, saving model to ../saved_models//meli-BERTK-v2_2-190919_1832/model.hdf5\n",
      "497500/497500 [==============================] - 4485s 9ms/sample - loss: 4.6595 - val_loss: 3.0376\n",
      "Epoch 2/6\n",
      "497408/497500 [============================>.] - ETA: 0s - loss: 2.7457\n",
      "Epoch 00002: val_loss improved from 3.03765 to 2.23158, saving model to ../saved_models//meli-BERTK-v2_2-190919_1832/model.hdf5\n",
      "497500/497500 [==============================] - 4455s 9ms/sample - loss: 2.7458 - val_loss: 2.2316\n",
      "Epoch 3/6\n",
      "497408/497500 [============================>.] - ETA: 0s - loss: 2.3118\n",
      "Epoch 00003: val_loss improved from 2.23158 to 1.99840, saving model to ../saved_models//meli-BERTK-v2_2-190919_1832/model.hdf5\n",
      "497500/497500 [==============================] - 4485s 9ms/sample - loss: 2.3118 - val_loss: 1.9984\n",
      "Epoch 4/6\n",
      "  5504/497500 [..............................] - ETA: 1:14:36 - loss: 2.1239"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-bd75f73cccac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVALIDATION_SET_SPLIT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m           callbacks=[tensorboard, checkpoint])\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "tensorboard = TensorBoard(log_dir=LOG_DIR+'/'+runid)\n",
    "checkpoint = ModelCheckpoint(directory+\"/model.hdf5\", monitor='val_loss',\n",
    "                             verbose=1, save_best_only=True, mode=\"min\")\n",
    "\n",
    "# train\n",
    "t0 = time.time()\n",
    "print(\"Start:\"+time.strftime(\"%Y%m%d_%H%M\",time.localtime()))\n",
    "model.fit(\n",
    "    [data['input_ids'][:num_training_samples], \n",
    "     data['input_masks'][:num_training_samples], \n",
    "     data['segment_ids'][:num_training_samples]], #input data \n",
    "          data['labels'][:num_training_samples], #labels\n",
    "          batch_size=128,#448, #384\n",
    "          epochs=6,\n",
    "          initial_epoch=0,\n",
    "          validation_split=VALIDATION_SET_SPLIT,\n",
    "          verbose=1,\n",
    "          callbacks=[tensorboard, checkpoint])\n",
    "\n",
    "tfin = time.time()\n",
    "print(\"End:\" + time.strftime(\"%Y%m%d_%H%M\",time.localtime()))\n",
    "print(tfin-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 40s 8ms/sample\n"
     ]
    }
   ],
   "source": [
    "#analysis_model = load_model(directory+\"/model.hdf5\", \n",
    "#                           custom_objects={'BertLayer':BertLayer})\n",
    "#print(\"Predict:\")\n",
    "predictions = model.predict(\n",
    "    [data['input_ids'][-num_test_samples:] , \n",
    "     data['input_masks'][-num_test_samples:], \n",
    "     data['segment_ids'][-num_test_samples:]], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BOOKS'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=np.argmax(predictions[0])\n",
    "data['inv_cat_dict'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FACIAL_SKIN_CARE_PRODUCTS'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it=data['labels'][-num_test_samples:].iloc[0]\n",
    "data['inv_cat_dict'][it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BOOKS',\n",
       " 'UKULELES',\n",
       " 'FACIAL_SKIN_CARE_PRODUCTS',\n",
       " 'WHISKEYS',\n",
       " 'HARD_DRIVES_AND_SSDS',\n",
       " 'AIRSOFT_GUNS',\n",
       " 'AM_FM_RADIOS',\n",
       " 'LIPSTICKS',\n",
       " 'DIECAST_VEHICLES',\n",
       " 'STARTERS']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_categories = [data['inv_cat_dict'][np.argmax(p)] for p in predictions]\n",
    "predicted_categories[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FACIAL_SKIN_CARE_PRODUCTS',\n",
       " 'UKULELES',\n",
       " 'SCULPTURES',\n",
       " 'WHISKEYS',\n",
       " 'CELLPHONE_COVERS',\n",
       " 'AIRSOFT_GUNS',\n",
       " 'AM_FM_RADIOS',\n",
       " 'LIPSTICKS',\n",
       " 'CAMERA_CHARGERS',\n",
       " 'SOLDERING_MACHINES']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_categories = [data['inv_cat_dict'][np.argmax(p)] for p in data['labels'][-num_test_samples:]]\n",
    "test_categories = [data['inv_cat_dict'][p] for p in list(data['labels'][-num_test_samples:])]\n",
    "test_categories[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>FACIAL_SKIN_CARE_PRODUCTS</td>\n",
       "      <td>BOOKS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>UKULELES</td>\n",
       "      <td>UKULELES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>SCULPTURES</td>\n",
       "      <td>FACIAL_SKIN_CARE_PRODUCTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4997</td>\n",
       "      <td>DRONES</td>\n",
       "      <td>OPERATING_SYSTEMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4998</td>\n",
       "      <td>HAIR_CLIPPERS</td>\n",
       "      <td>HAIR_CLIPPERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4999</td>\n",
       "      <td>HAIR_CLIPPERS</td>\n",
       "      <td>CAMERA_TRIPODS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       category                  predicted\n",
       "0     FACIAL_SKIN_CARE_PRODUCTS                      BOOKS\n",
       "1                      UKULELES                   UKULELES\n",
       "2                    SCULPTURES  FACIAL_SKIN_CARE_PRODUCTS\n",
       "...                         ...                        ...\n",
       "4997                     DRONES          OPERATING_SYSTEMS\n",
       "4998              HAIR_CLIPPERS              HAIR_CLIPPERS\n",
       "4999              HAIR_CLIPPERS             CAMERA_TRIPODS\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.DataFrame({'category':test_categories, 'predicted':predicted_categories})\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3018"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[\"true_positive\"] = test_set[\"category\"]==test_set[\"predicted\"]\n",
    "test_set[\"true_positive\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>predicted</th>\n",
       "      <th>true_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>FACIAL_SKIN_CARE_PRODUCTS</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>UKULELES</td>\n",
       "      <td>UKULELES</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>SCULPTURES</td>\n",
       "      <td>FACIAL_SKIN_CARE_PRODUCTS</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>LIPSTICKS</td>\n",
       "      <td>LIPSTICKS</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>CAMERA_CHARGERS</td>\n",
       "      <td>DIECAST_VEHICLES</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>SOLDERING_MACHINES</td>\n",
       "      <td>STARTERS</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     category                  predicted  true_positive\n",
       "0   FACIAL_SKIN_CARE_PRODUCTS                      BOOKS          False\n",
       "1                    UKULELES                   UKULELES           True\n",
       "2                  SCULPTURES  FACIAL_SKIN_CARE_PRODUCTS          False\n",
       "..                        ...                        ...            ...\n",
       "7                   LIPSTICKS                  LIPSTICKS           True\n",
       "8             CAMERA_CHARGERS           DIECAST_VEHICLES          False\n",
       "9          SOLDERING_MACHINES                   STARTERS          False\n",
       "\n",
       "[10 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4038453218909564"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_accuracy_score(test_set['category'], test_set['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.to_csv(directory+\"/results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
