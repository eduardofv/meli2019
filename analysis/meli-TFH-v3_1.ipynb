{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MercadoLibre Challenge \n",
    "### TensorflowHub embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"meli-TFH\" \n",
    "EXPERIMENT_VERSION = \"v3_1\"\n",
    "LOG_DIR = \"../logs/TFH-3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version log\n",
    "\n",
    "### TFH-v3_1\n",
    "\n",
    "Es la versión 3, final del challenge, ejecutada en GPU de muestra con el objetivo de tener una versión en Jupyter que se pueda compartir y analizar mas fácilmente. Se cambió a 512 unidades en las capas densas y batches de 256 para poder entrenar en GTX960M por 3 épocas\n",
    "\n",
    "### TFH-v2_3\n",
    "Viene de 2_2_2\n",
    "\n",
    "### TFH-v2_2\n",
    "\n",
    "Ajustar hiperparámetros para buscar mejor score\n",
    "\n",
    "### TFH-v2_1\n",
    "\n",
    "Usar normalización\n",
    "\n",
    "### TFH-v2_0\n",
    "\n",
    "Nueva arquitectura, incorporarar info de lenguaje\n",
    "\n",
    "Viene de meli-TFH-v1_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1' #disable gpu\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import balanced_accuracy_score \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout\n",
    "from tensorflow.keras.layers import Concatenate, Input\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"TF Version: {tf.__version__}\")\n",
    "#print(f\"TF Hub Version: {hub.__version__}\")\n",
    "assert( tf.__version__ >= \"2.0.0-beta0\" ) #rc1\n",
    "assert( hub.__version__ >= \"0.3\" ) #\"0.7.0.dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize env\n",
    "#seeds to make reproducible\n",
    "#todo: check reproducibility\n",
    "np.random.seed(12347)\n",
    "tf.random.set_seed(12347)\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "#filenames and directories\n",
    "DATASET_FN = \"../data/train.csv\"\n",
    "ROWS_TO_LOAD = None#4000000 #None == all\n",
    "SAVED_MODEL_DIR = \"../saved_models/\"\n",
    "\n",
    "#TFHUB_EMB_MODEL = \"https://tfhub.dev/google/universal-sentence-encoder/2\" \n",
    "# download the module manually if network problems \n",
    "# like URLError: <urlopen error [Errno -3] Temporary failure in name resolution>\n",
    "# check https://www.tensorflow.org/hub/common_issues \n",
    "#TFHUB_EMB_MODEL = \"../tf_hub_cache/gnews-swivel-20dim-v1\"\n",
    "#TFHUB_EMB_MODEL = \"../tf_hub_cache/nnlm-es-dim50\" #\n",
    "#TFHUB_EMB_MODEL = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n",
    "#TFHUB_EMB_MODEL = \"https://tfhub.dev/google/tf2-preview/nnlm-es-dim128/1\"\n",
    "#TFHUB_EMB_MODEL = \"../tf_hub_cache/nnlm-es-dim128-with-normalization\"\n",
    "TFHUB_EMB_MODEL = \"https://tfhub.dev/google/tf2-preview/nnlm-es-dim128-with-normalization/1\"\n",
    "TFHUB_EMB_MODEL_DIM = 128\n",
    "LANGUAGE = \"spanish\" #\"portuguese\" #None\n",
    "\n",
    "MODEL_DENSE_UNITS = 256\n",
    "MODEL_DROPOUT_RATE = 0.5\n",
    "\n",
    "#Training\n",
    "TRAIN_EPOCHS = 3\n",
    "TRAIN_INITIAL_EPOCH = 0\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "\n",
    "#set some parameters on how data will be used\n",
    "# specify a small proportion to speed things while testing, 1.0 when running full training\n",
    "DATASET_PROPORTION_TO_USE = 1.0\n",
    "# how much data will reserve for test set (of the DS prop to use) (0.10)\n",
    "TEST_SET_SPLIT = 0.004\n",
    "# how much of the data will be used for validation (of the DS prop to use) (0.05)\n",
    "VALIDATION_SET_SPLIT = 0.004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      title label_quality  \\\n",
      "0         Hidrolavadora Lavor One 120 Bar 1700w  Bomba A...    unreliable   \n",
      "1                         Placa De Sonido - Behringer Umc22    unreliable   \n",
      "4         Flashes Led Pestañas Luminoso Falso Pestañas P...    unreliable   \n",
      "9                                 Gatito Lunchera Neoprene     unreliable   \n",
      "11        Rosario Contador De Billetes Uv / Mg Detecta F...    unreliable   \n",
      "...                                                     ...           ...   \n",
      "19999993      + Frazada Termica Antipiling 2 P Jean Cartier    unreliable   \n",
      "19999994                 Pijama Homero Simpson Mujer Verano    unreliable   \n",
      "19999995     Brochas De Maquillaje Kylie Set De 12 Unidades    unreliable   \n",
      "19999996       Trimmer Detailer Wahl + Kit Tijeras Stylecut      reliable   \n",
      "19999998                 Palo De Hockey Grays Nano 7 37,5''    unreliable   \n",
      "\n",
      "         language                   category  \n",
      "0         spanish  ELECTRIC_PRESSURE_WASHERS  \n",
      "1         spanish                SOUND_CARDS  \n",
      "4         spanish            FALSE_EYELASHES  \n",
      "9         spanish                 LUNCHBOXES  \n",
      "11        spanish              BILL_COUNTERS  \n",
      "...           ...                        ...  \n",
      "19999993  spanish                   BLANKETS  \n",
      "19999994  spanish                    PAJAMAS  \n",
      "19999995  spanish             MAKEUP_BRUSHES  \n",
      "19999996  spanish              HAIR_CLIPPERS  \n",
      "19999998  spanish        FIELD_HOCKEY_STICKS  \n",
      "\n",
      "[10000000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATASET_FN, nrows=ROWS_TO_LOAD)\n",
    "if LANGUAGE is not None:\n",
    "    df = df[df[\"language\"]==LANGUAGE]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1574\n",
      "['ELECTRIC_PRESSURE_WASHERS', 'SOUND_CARDS', 'FALSE_EYELASHES']\n"
     ]
    }
   ],
   "source": [
    "output_dim = len(df[\"category\"].unique())\n",
    "print(output_dim)\n",
    "print(list(df[\"category\"][:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "4    2\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_dict = dict(zip(df[\"category\"].unique(), np.arange(output_dim)))\n",
    "inverse_cat_dict = dict(zip(cat_dict.values(), cat_dict.keys()))\n",
    "labels = df[\"category\"].map(cat_dict)\n",
    "#labels = to_categorical(df[\"category\"].map(cat_dict))\n",
    "labels[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = (df[\"language\"]==LANGUAGE).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9960000,)\n",
      "(9960000,)\n",
      "(40000,)\n",
      "(40000,)\n"
     ]
    }
   ],
   "source": [
    "num_samples = len(df)\n",
    "num_test_samples = int(num_samples * TEST_SET_SPLIT)\n",
    "num_training_samples = num_samples - num_test_samples\n",
    "\n",
    "training_set_data = df[\"title\"].head(num_training_samples)\n",
    "training_set_lang = language[:num_training_samples]\n",
    "training_set_labels = labels[:num_training_samples]\n",
    "test_set_data = df[\"title\"].tail(num_test_samples)\n",
    "test_set_lang = language[-num_test_samples:]\n",
    "test_set_labels = labels[-num_test_samples:]\n",
    "\n",
    "print(training_set_data.shape)\n",
    "print(training_set_labels.shape)\n",
    "print(test_set_data.shape)\n",
    "print(test_set_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "keras_layer_input (InputLayer)  [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        (None, 128)          125009920   keras_layer_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "DEN_1 (Dense)                   (None, 256)          33024       keras_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           DEN_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "DEN_2 (Dense)                   (None, 256)          65792       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           DEN_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 257)          0           dropout_1[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "DEN_OUT (Dense)                 (None, 1574)         406092      concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 125,514,828\n",
      "Trainable params: 125,514,828\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "hub_layer = hub.KerasLayer(TFHUB_EMB_MODEL,\n",
    "                    input_shape=[],\n",
    "                    output_shape=[TFHUB_EMB_MODEL_DIM],\n",
    "                    trainable=True, \n",
    "                    dtype=tf.string)\n",
    "\n",
    "input_lang = Input(shape=(1,))\n",
    "\n",
    "mod = Sequential()\n",
    "mod.add(hub_layer)\n",
    "\n",
    "fo = Dense(MODEL_DENSE_UNITS, activation=\"relu\", name=\"DEN_1\")(mod.output)\n",
    "fo = Dropout(rate=MODEL_DROPOUT_RATE)(fo)\n",
    "fo = Dense(MODEL_DENSE_UNITS, activation=\"relu\", name=\"DEN_2\")(fo)\n",
    "fo = Dropout(rate=MODEL_DROPOUT_RATE)(fo)\n",
    "fo = Concatenate()([fo, input_lang])\n",
    "fo = Dense(output_dim, activation=\"softmax\", name=\"DEN_OUT\")(fo)\n",
    "model = Model(inputs=[mod.input, input_lang], outputs=fo)\n",
    "\n",
    "optimizer = Adam()\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='sparse_categorical_crossentropy')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.5.2 (default, Nov 12 2018, 13:43:14) \n",
      "[GCC 5.4.0 20160609]\n",
      "Tensorflow version: 2.0.0-beta0\n",
      "Keras version: 2.2.4-tf\n",
      "RUNID: meli-TFH-v3_1-191008_1628\n"
     ]
    }
   ],
   "source": [
    "#run params\n",
    "runid = \"%s-%s-%s\"%(EXPERIMENT_NAME,\n",
    "     EXPERIMENT_VERSION,\n",
    "     time.strftime(time.strftime('%y%m%d_%H%M',time.localtime())))\n",
    "\n",
    "#Create saved model dir     \n",
    "directory = SAVED_MODEL_DIR+\"/\"+runid\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "print(\"Python: \"+str(sys.version))\n",
    "print(\"Tensorflow version: \"+tf.__version__)\n",
    "print(\"Keras version: \"+tf.keras.__version__)\n",
    "print(\"RUNID: \"+runid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1008 16:28:45.276495 140514687506176 deprecation.py:323] From /.local/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Start:20191008_1628\n",
      "Train on 9920160 samples, validate on 39840 samples\n",
      "Epoch 1/3\n",
      "    128/9920160 [..............................] - ETA: 25:10:42 - loss: 7.3627"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[976640,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/embedding_lookup_sparse/embedding_lookup/Read/ReadVariableOp}}]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  476.88MiB from Adam/Adam/update/ReadVariableOp_6\n  476.88MiB from Adam/Adam/update/ReadVariableOp_3\n  Remaining 1 nodes with 1.0KiB\n\n\t [[dropout_1/cond/then/_22/dropout/mul/_72]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  476.88MiB from Adam/Adam/update/ReadVariableOp_6\n  476.88MiB from Adam/Adam/update/ReadVariableOp_3\n  Remaining 1 nodes with 1.0KiB\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[976640,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/embedding_lookup_sparse/embedding_lookup/Read/ReadVariableOp}}]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  476.88MiB from Adam/Adam/update/ReadVariableOp_6\n  476.88MiB from Adam/Adam/update/ReadVariableOp_3\n  Remaining 1 nodes with 1.0KiB\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_keras_scratch_graph_1300]\n\nFunction call stack:\nkeras_scratch_graph -> keras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2c677fde1155>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVALIDATION_SET_SPLIT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m           callbacks=[tensorboard, checkpoint])\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~.local/lib/python3.5/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~.local/lib/python3.5/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~.local/lib/python3.5/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~.local/lib/python3.5/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[976640,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/embedding_lookup_sparse/embedding_lookup/Read/ReadVariableOp}}]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  476.88MiB from Adam/Adam/update/ReadVariableOp_6\n  476.88MiB from Adam/Adam/update/ReadVariableOp_3\n  Remaining 1 nodes with 1.0KiB\n\n\t [[dropout_1/cond/then/_22/dropout/mul/_72]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  476.88MiB from Adam/Adam/update/ReadVariableOp_6\n  476.88MiB from Adam/Adam/update/ReadVariableOp_3\n  Remaining 1 nodes with 1.0KiB\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[976640,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/embedding_lookup_sparse/embedding_lookup/Read/ReadVariableOp}}]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  476.88MiB from Adam/Adam/update/ReadVariableOp_6\n  476.88MiB from Adam/Adam/update/ReadVariableOp_3\n  Remaining 1 nodes with 1.0KiB\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_keras_scratch_graph_1300]\n\nFunction call stack:\nkeras_scratch_graph -> keras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "#Train from saved point\n",
    "#model = tf.keras.models.load_model(\"../saved_models/meli-TFH-v3-191001_0150/model.hdf5\", \n",
    "#                                            custom_objects={\"KerasLayer\":hub_layer})\n",
    "\n",
    "print('Training...')\n",
    "tensorboard = TensorBoard(log_dir=LOG_DIR+'/'+runid)\n",
    "checkpoint = ModelCheckpoint(directory+\"/model.hdf5\", monitor='val_loss',\n",
    "                             verbose=1, save_best_only=True, mode=\"min\")\n",
    "\n",
    "# train\n",
    "t0 = time.time()\n",
    "print(\"Start:\"+time.strftime(\"%Y%m%d_%H%M\",time.localtime()))\n",
    "history = model.fit([training_set_data.array, training_set_lang.array], \n",
    "          training_set_labels,\n",
    "          batch_size=TRAIN_BATCH_SIZE,\n",
    "          epochs=TRAIN_EPOCHS,\n",
    "          initial_epoch=TRAIN_INITIAL_EPOCH,\n",
    "          validation_split=VALIDATION_SET_SPLIT,\n",
    "          verbose=1,\n",
    "          callbacks=[tensorboard, checkpoint])\n",
    "\n",
    "tfin = time.time()\n",
    "print(\"End:\" + time.strftime(\"%Y%m%d_%H%M\",time.localtime()))\n",
    "print(tfin-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = directory+\"/history.pickle\"\n",
    "with open(fn, \"wb\")as fo:\n",
    "    pickle.dump(history.history, fo, protocol=4)\n",
    "print(f\"Saved {fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last Model (current)\n",
    "#analysis_model = model\n",
    "\n",
    "#Best Model saved\n",
    "analysis_model = tf.keras.models.load_model(directory+\"/model.hdf5\", \n",
    "                                            custom_objects={\"KerasLayer\":hub_layer})\n",
    "\n",
    "print(\"Predict:\")\n",
    "predictions = analysis_model.predict([test_set_data.array, \n",
    "                                      test_set_lang.array], verbose=1)\n",
    "\n",
    "inverse_cat_dict = dict(zip(cat_dict.values(), cat_dict.keys()))\n",
    "predicted_categories = [inverse_cat_dict[np.argmax(p)] for p in predictions]\n",
    "\n",
    "real_categories = [inverse_cat_dict[p] for p in test_set_labels]\n",
    "\n",
    "bac = balanced_accuracy_score(real_categories, predicted_categories)\n",
    "print(f\"Eval BAC={bac}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### History graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(directory+\"/history.pickle\", \"rb\") as f:\n",
    "#    history = pickle.load(f)\n",
    "history_dict = history.history #current\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "fn = directory+\"/history.pdf\"\n",
    "plt.show()\n",
    "#Save\n",
    "#plt.savefig(fn)\n",
    "#print(f\"Saved {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_test_df = pd.read_csv(\"../data/test.csv\")\n",
    "#submission_test_df\n",
    "sub_data = submission_test_df[\"title\"]\n",
    "sub_lang = (submission_test_df[\"language\"]=='spanish').astype(\"int32\")\n",
    "\n",
    "sub_predictions = analysis_model.predict([sub_data.array, sub_lang.array], verbose=1)\n",
    "sub_predicted_categories = [inverse_cat_dict[np.argmax(p)] for p in sub_predictions]\n",
    "#sub_predicted_categories[:10]\n",
    "submission = pd.DataFrame({\"id\":range(len(sub_predicted_categories)), \n",
    "                          \"category\":sub_predicted_categories})\n",
    "#submission\n",
    "fn = directory+\"/submission-\"+runid+\".csv\"\n",
    "submission[[\"id\",\"category\"]].to_csv(fn, index=False)\n",
    "print(f\"Saved {fn}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
